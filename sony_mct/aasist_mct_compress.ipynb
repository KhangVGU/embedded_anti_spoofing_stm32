{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ1NYLjaZopUNBSSwgj+Nd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhangVGU/embedded_anti_spoofing_stm32/blob/main/sony_mct/aasist_mct_compress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aN7-vDmPFgQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e24eee-503c-41f6-b806-ba965bad0ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'aasist' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/clovaai/aasist.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/aasist/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ZQZrny2hFv4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49b7fdc-73e1-4ef9-e204-612d84ac3590"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/aasist\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchcontrib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->-r requirements.txt (line 4)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 4)) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->-r requirements.txt (line 1)) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./download_dataset.py"
      ],
      "metadata": {
        "id": "6BjLT57sJbaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook parameters\n",
        "config_path     = \"/content/aasist/config/AASIST.conf\"\n",
        "checkpoint_path = \"/content/aasist/models/weights/AASIST.pth\"\n",
        "audio_dir       = \"/content/aasist/LA/ASVspoof2019_LA_dev/flac\"\n",
        "n_iter          = 20\n",
        "batch_size      = 1"
      ],
      "metadata": {
        "id": "ndpp8RDbEq_c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install required packages\n",
        "!pip install -q torch torchvision onnx tqdm model_compression_toolkit soundfile scipy"
      ],
      "metadata": {
        "id": "ODCQwFpsEsXr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/aasist/\n",
        "# 2) Imports\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from scipy.signal import resample\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import model_compression_toolkit as mct\n",
        "from AASIST import Model\n",
        "from utils import str_to_bool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhDSTVbgE_QE",
        "outputId": "46f79a63-99ed-4ab0-f991-70e1e1b3e1af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/aasist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Audio loading & dataset\n",
        "def load_audio(path: str, target_sr: int = 16000) -> np.ndarray:\n",
        "    wav, sr = sf.read(path)\n",
        "    if sr != target_sr:\n",
        "        num = int(len(wav) * target_sr / sr)\n",
        "        wav = resample(wav, num)\n",
        "    return wav.astype(np.float32)\n",
        "\n",
        "class AudioFolderDataset(Dataset):\n",
        "    \"\"\"All .flac files under audio_dir.\"\"\"\n",
        "    def __init__(self, root_dir, target_sr=16000):\n",
        "        self.paths = sorted([\n",
        "            os.path.join(root_dir, fn)\n",
        "            for fn in os.listdir(root_dir)\n",
        "            if fn.lower().endswith('.flac')\n",
        "        ])\n",
        "        self.target_sr = target_sr\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav = load_audio(self.paths[idx], self.target_sr)\n",
        "        return torch.from_numpy(wav), self.paths[idx]\n",
        "\n",
        "def representative_dataset_gen(dataset, batch_size=1, n_iter=20):\n",
        "    \"\"\"Yield lists of numpy arrays for MCT.\"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    it = iter(loader)\n",
        "    for _ in range(n_iter):\n",
        "        try:\n",
        "            batch, _ = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            batch, _ = next(it)\n",
        "        yield [batch.cpu().numpy()]\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"Run inference and print per‐file predictions.\"\"\"\n",
        "    model.to(device).eval()\n",
        "    with torch.no_grad():\n",
        "        for x, path in tqdm(dataloader, desc=\"Eval\"):\n",
        "            x = x.to(device)\n",
        "            _, logits = model(x, Freq_aug=False)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            pred = probs.argmax(dim=-1).item()\n",
        "            label = {0: 'Spoof', 1: 'Bona fide'}.get(pred, 'Unknown')\n",
        "            print(f\"{os.path.basename(path[0])}: {label} ({probs[0,pred]:.3f})\")"
      ],
      "metadata": {
        "id": "Ma8cM_rBFA_E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Load config, instantiate & load checkpoint\n",
        "with open(config_path, 'r') as f:\n",
        "    cfg = json.load(f)\n",
        "d_args = cfg['model_config']\n",
        "\n",
        "if not isinstance(d_args['filts'][0], int):\n",
        "    raise ValueError(\"d_args['filts'][0] must be int\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = Model(d_args).to(device)\n",
        "ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "sd = ckpt.get('state_dict', ckpt)\n",
        "res = model.load_state_dict(sd, strict=False)\n",
        "if res.unexpected_keys: print(\"Unexpected keys:\", res.unexpected_keys)\n",
        "if res.missing_keys:    print(\"Missing keys:\",    res.missing_keys)\n",
        "\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KORpB_wFG39",
        "outputId": "bcdff8db-32fb-43f9-9a7a-912a33d1946e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7a72050c2010>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Prepare datasets\n",
        "dataset     = AudioFolderDataset(audio_dir, target_sr=16000)\n",
        "rep_gen     = lambda: representative_dataset_gen(dataset, batch_size, n_iter)\n",
        "eval_loader = DataLoader(dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "KdqsLcOPGAHD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) GPTQ config & target platform\n",
        "gptq_config = mct.gptq.get_pytorch_gptq_config(n_epochs=50)\n",
        "tpc = mct.get_target_platform_capabilities(\n",
        "    'pytorch',\n",
        "    'tflite',\n",
        "    target_platform_version=\"v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZHNvDMXDGBAL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Run quantization (may take several minutes)\n",
        "print(\"Starting GPTQ quantization…\")\n",
        "q_model, q_info = mct.gptq.pytorch_gradient_post_training_quantization(\n",
        "    model,\n",
        "    rep_gen,\n",
        "    gptq_config=gptq_config,\n",
        "    target_platform_capabilities=tpc\n",
        ")\n",
        "print(\"Quantization done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pC0pmIHJGcgN",
        "outputId": "b2073bb1-fc93-4ca7-f3f3-602b004b7968"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:Model Compression Toolkit:representative_data_gen generates a batch size of 1 which can be slow for optimization: consider increasing the batch size\n",
            "CRITICAL:Model Compression Toolkit:Error parsing model with torch.fx\n",
            "fx error: symbolically traced variables cannot be used as inputs to control flow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GPTQ quantization…\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Error parsing model with torch.fx\nfx error: symbolically traced variables cannot be used as inputs to control flow",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/pytorch/reader/reader.py\u001b[0m in \u001b[0;36mfx_graph_module_generation\u001b[0;34m(pytorch_model, representative_data_gen, to_tensor)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0msymbolic_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/pytorch/reader/reader.py\u001b[0m in \u001b[0;36m_trace_model\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# handling the possibility that the model (root) might be a torch.nn.Module or a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    842\u001b[0m                     \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                     \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/aasist/AASIST.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, Freq_aug)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# x = x.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFreq_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mmodule_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m                 )\n\u001b[0;32m--> 821\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mcall_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_qualified_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                 \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0m_orig_module_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/aasist/AASIST.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mband_pass_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mband_pass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/proxy.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/proxy.py\u001b[0m in \u001b[0;36mto_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \"\"\"\n\u001b[0;32m--> 360\u001b[0;31m         raise TraceError(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;34m\"symbolically traced variables cannot be used as inputs to control flow\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-4142431468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 7) Run quantization (may take several minutes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting GPTQ quantization…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m q_model, q_info = mct.gptq.pytorch_gradient_post_training_quantization(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrep_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/gptq/pytorch/quantization_facade.py\u001b[0m in \u001b[0;36mpytorch_gradient_post_training_quantization\u001b[0;34m(model, representative_data_gen, target_resource_utilization, core_config, gptq_config, gptq_representative_data_gen, target_platform_capabilities)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Core Runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# ---------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         graph, bit_widths_config, hessian_info_service, scheduling_info = core_runner(in_model=model,\n\u001b[0m\u001b[1;32m    234\u001b[0m                                                                                       \u001b[0mrepresentative_data_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepresentative_data_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                                                                                       \u001b[0mcore_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcore_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/runner.py\u001b[0m in \u001b[0;36mcore_runner\u001b[0;34m(in_model, representative_data_gen, core_config, fw_info, fw_impl, fqc, target_resource_utilization, running_gptq, tb_w)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mixed precision enabled.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     graph = graph_preparation_runner(in_model,\n\u001b[0m\u001b[1;32m    100\u001b[0m                                      \u001b[0mrepresentative_data_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                                      \u001b[0mcore_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/graph_prep_runner.py\u001b[0m in \u001b[0;36mgraph_preparation_runner\u001b[0;34m(in_model, representative_data_gen, quantization_config, fw_info, fw_impl, fqc, bit_width_config, tb_w, mixed_precision_enable, running_gptq)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     graph = read_model_to_graph(in_model,\n\u001b[0m\u001b[1;32m     74\u001b[0m                                 \u001b[0mrepresentative_data_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                 \u001b[0mfqc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/graph_prep_runner.py\u001b[0m in \u001b[0;36mread_model_to_graph\u001b[0;34m(in_model, representative_data_gen, fqc, fw_info, fw_impl)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mGraph\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mrepresents\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \"\"\"\n\u001b[0;32m--> 211\u001b[0;31m     graph = fw_impl.model_reader(in_model,\n\u001b[0m\u001b[1;32m    212\u001b[0m                                  representative_data_gen)\n\u001b[1;32m    213\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_fw_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/pytorch/pytorch_implementation.py\u001b[0m in \u001b[0;36mmodel_reader\u001b[0;34m(self, module, representative_data_gen)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0m_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0m_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentative_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     def model_builder(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/pytorch/reader/reader.py\u001b[0m in \u001b[0;36mmodel_reader\u001b[0;34m(model, representative_data_gen, to_numpy, to_tensor)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \"\"\"\n\u001b[1;32m    173\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start Model Reading...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mfx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfx_graph_module_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentative_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mdisconnected_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/core/pytorch/reader/reader.py\u001b[0m in \u001b[0;36mfx_graph_module_generation\u001b[0;34m(pytorch_model, representative_data_gen, to_tensor)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msymbolic_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         Logger.critical(f'Error parsing model with torch.fx\\n'\n\u001b[0m\u001b[1;32m    114\u001b[0m                         f'fx error: {e}')\n\u001b[1;32m    115\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentative_data_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/model_compression_toolkit/logger.py\u001b[0m in \u001b[0;36mcritical\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \"\"\"\n\u001b[1;32m    116\u001b[0m         \u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Error parsing model with torch.fx\nfx error: symbolically traced variables cannot be used as inputs to control flow"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Evaluate original vs. quantized\n",
        "print(\"\\nFloating-point model:\")\n",
        "evaluate(model,     eval_loader, device)\n",
        "print(\"\\nQuantized model:\")\n",
        "evaluate(q_model, eval_loader, device)"
      ],
      "metadata": {
        "id": "PCsZA4uBG4Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Export quantized model to ONNX\n",
        "print(\"\\nExporting to qmodel.onnx …\")\n",
        "mct.exporter.pytorch_export_model(\n",
        "    q_model,\n",
        "    save_model_path='qmodel.onnx',\n",
        "    repr_dataset=rep_gen\n",
        ")\n",
        "print(\"Export complete.\")"
      ],
      "metadata": {
        "id": "M2t6KUNEG6or"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}